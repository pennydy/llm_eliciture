# -*- coding: utf-8 -*-
"""rt_preprocess

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12OiJDOD3cZ9uo9itVZ4sSg8-vX9iyXkQ
"""

import pandas as pd
import ast

model_results = pd.read_csv('/content/rc_sent_alt_pythia-1b_2.csv', header=0)
# only use the first 80 rows that have corresponding human results
model_results = model_results.head(80)

# Create the 'word_prob' column and initialize it with empty lists
model_results['word_prob'] = [[] for _ in range(len(model_results))]
participles = [" near", " with", " to", " next", " into", " at", " policy",
               " grad", " dollars", " waste", " spot", " favor"] # text_policy
determiners = [" the", " a"] # the_teacher
verbs = [" go", " willing", " late"] # willing to_spot

for i, row in model_results.iterrows():
  logprob = list(ast.literal_eval(row.logprob))
  sentence_prob = {}
  previous_word = ""
  for j, word_prob in enumerate(logprob):
    word = word_prob[0]
    prob = word_prob[1]
    if previous_word == " in_favor":
      word = " of_1" # the of is a seperate
    if (" " in word or j==0) and previous_word not in determiners and word not in participles:
      sentence_prob[word.strip()] = prob
      # the previous word is the combination of two or more tokens
      # sum the surprisals
      # sentence_prob[previous_word.strip()] = sentence_prob[previous_word.strip()] / num_previous_word
      previous_word = word
    else:
      # sometimes "to" is being combined with the word after
      if previous_word in verbs:
        sentence_prob[word.strip()] = prob
        previous_word = word
        continue
      # "the school" is seperated into two
      if previous_word == " the" and word == " school":
        sentence_prob[word.strip()] = prob
        previous_word = word
        continue

      # other cases: combine the two words
      previous_prob = sentence_prob[previous_word.strip()]
      # combine the with the following noun, matching the critical region
      if previous_word in determiners or word in participles:
        new_word = previous_word + "_" + word.lstrip(' ')
      else:
        new_word = previous_word + word
      sentence_prob[new_word.strip()] = sentence_prob.pop(previous_word.strip())

      sentence_prob[new_word.strip()] = prob + previous_prob
      previous_word = new_word

  model_results.at[i, "word_prob"] = list(sentence_prob.items())
  print(list(sentence_prob.items()))

# one word per row
model_results = model_results.explode('word_prob')

model_results = model_results.reset_index(drop=True)
model_results["sentence_type"]=model_results["sentence_type"].str.lower()

# get the prob of each word -- there should be an easier way to do this
for i, row in model_results.iterrows():
  word = row.word_prob[0]
  prob = row.word_prob[1]
  model_results.at[i, "word"] = word
  model_results.at[i, "prob"] = prob

# keep the of_1 so that it won't be combined with the "of"
# model_results["word"] = model_results["word"].replace("of_1", "of")

model_results.to_csv('model_results.csv', index=False)

frequency_data = pd.read_csv('/content/eliciture_frequency.csv', header=0)
human_results = pd.read_csv('/content/human_results.csv', header=0)

combined_results = pd.merge(human_results, model_results, how='left',
                            on=['word','cond','attachment','sentence_type',
                                'item_id'])
combined_frequency_results = combined_results.merge(frequency_data, how='left',
                            on='word')

combined_frequency_results.to_csv('rc_rt_combined_pythia-1b_2.csv', index=False)